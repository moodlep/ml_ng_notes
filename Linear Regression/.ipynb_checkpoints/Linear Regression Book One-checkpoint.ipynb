{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "\n",
    "Adding math symbols using Latex (mathjax)\n",
    "http://www.martinkeefe.com/math/mathjax5\n",
    "http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb#4.9-LaTeX-Math\n",
    "\n",
    "Andrew Ng Course Notes: \n",
    "http://cs229.stanford.edu/materials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Linear Regression from Coursera ML course - Andrew Ng\n",
    "\n",
    "## Model: \n",
    "\n",
    "### Data in the dataset: \n",
    "X feature matrix $(m x n)$ \n",
    "    where m = no of training examples (i = 1 to m)\n",
    "          and n is the number of features (j = 0 to n) ** notice the 0 element here ** \n",
    "          \n",
    "X may be re-written as feature vectors: \n",
    "\n",
    "$ X = \\begin{bmatrix} (x^{(1)})^T \\\\ (x^{(2)})^T \\\\ ... \\\\ (x^{(m)})^T  \\end{bmatrix} $\n",
    "\n",
    "where each $x^{(i)} $ is the vector of n features for the row i and \n",
    "\n",
    "$ x^{(i)} = \\begin{bmatrix} x_0^{(i)} \\\\ x_1^{(i)} \\\\ ... \\\\ x_n^{(i)} \\end{bmatrix} $\n",
    "\n",
    "y vector of known outcomes/values $(m x 1)$ so $y(i)$ corresponds to $X(i,:)$\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "The hypothesis chosen is a linear function of the features\n",
    "\n",
    "$h(X) = X\\theta$\n",
    "\n",
    "where $\\theta$ is a weight for each feature. \n",
    "\n",
    "$ \\theta\\substack{nx1} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ ... \\\\ \\theta_n  \\end{bmatrix} $\n",
    "\n",
    "### The Cost Function \n",
    "\n",
    "The cost function $J(\\theta)$ is determined using the Least Squares method and is\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^i) - y^i)^2 $  or written in matrix format  $J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T (X\\theta - y)$\n",
    "\n",
    "$J(\\theta)$ is a convex quadratic function and has a global minimum in this case. \n",
    "(Refer to wikipedia here for convex quadratic functions: \n",
    "    *Well-known examples of convex functions are the quadratic function $x^2$ and the exponential function $e^x$ for any real number x.*\n",
    ")\n",
    "This means the derivative of $J(\\theta)$ at a point will be 0. Gradient descent is used to find the values of $\\theta$ that result in this minimum or optimum value of the cost, $J(\\theta)$. \n",
    "\n",
    "#### Gradient Descent\n",
    "Gradient descent to find $\\theta$: \n",
    "\n",
    "Repeat until convergence: \n",
    "    $\\theta = \\theta - \\frac{\\alpha}{m} (X^T (X\\theta - y)) $\n",
    "    where the term inside the bracket is the derivative of $J(\\theta)$\n",
    "    $\\alpha$ is the learning rate or the step size for the descent and affects the rate of convergence and in fact can cause divergence if the rate is too large. \n",
    "    \n",
    "A nice extract from Andrew Ng's notes follows: \n",
    "    \n",
    "$\\theta_j := \\theta_j + \\alpha(y^i - h_\\theta(x^i)) x_j^i $\n",
    "    \n",
    "    The rule is called the LMS update rule (LMS stands for “least mean squares”), and is also known as the Widrow-Hoff learning rule. This rule has several properties that seem natural and intuitive. For instance, the magnitude of the update is proportional to the error term (y(i) − hθ(x(i))); thus, for instance, if we are encountering a training example on which our prediction nearly matches the actual value of y(i), then we find that there is little need to change the parameters; in contrast, a larger change to the parameters will be made if our prediction hθ(x(i)) has a large error (i.e., if it is very far from y(i)).\n",
    "\n",
    "##### Stochastic vs batch gradient descent\n",
    "\n",
    "Batch gradient descent basically solves $\\theta_j$ using the entire training set: \n",
    "\n",
    "Repeat until convergence: {\n",
    "$\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^m(y^i - h_\\theta(x^i)) x_j^i $   for all j  }\n",
    "\n",
    "whereas stochastic gradient descent calculates $\\theta_j$ based on one training example at a time: \n",
    "\n",
    "Loop from i = 0 to m {\n",
    "$\\theta_j := \\theta_j + \\alpha(y^i - h_\\theta(x^i)) x_j^i $   for all j  }\n",
    "\n",
    "In the latter case the convergence occurs on every step and it can converge faster than batch although it may meandre or oscillate a bit. More useful than batch if the value of m is too high in which case batch becomes costly.  \n",
    "\n",
    "### The Normal Equation \n",
    "\n",
    "Aside from gradient descent, the normal equation maybe used to determine the optimal value of $\\theta$ relying on the fact that this cost function has a global minimum and therefore we may solve for \n",
    "\n",
    "   $\\frac{\\partial J}{\\partial\\theta} = 0$\n",
    "   \n",
    "   finding $\\theta = (X^T X)^{-1} X^T y$\n",
    "   \n",
    "### Gradient Descent vs Normal Equation\n",
    "   \n",
    "Gradient Descent | Normal Equation\n",
    "-|-\n",
    "Scaling or mean normalisation may be required| no scaling required\n",
    "Need to choose a learning rate | N/A\n",
    "iterative process | non-iterative\n",
    "Works well for large n > 10k | Better for n < 10k as $(X^TX)^-1$ becomes costly and slow\n",
    "\n",
    "   \n",
    "   \n",
    "### Practical advice\n",
    "\n",
    "Notes on solving for $\\theta$ using gradient descent\n",
    "\n",
    "* The learning rate $\\alpha$ influences convergence. If $\\alpha$ is too large the global minimum may be missed and divergence occurs. If the rate is too small convergence can take longer. \n",
    "    \n",
    "* A starting set of weights or initial guess for $\\theta$ is required to kick off gradient descent. Most implementations provide this initial set of values (TBC) \n",
    "    \n",
    "* The derivative of the cost function $J(\\theta)$ is required for gradient descent. The derivative is provided as a function to the implementation. \n",
    "    \n",
    "* feature scaling: if the features have vastly different scales it may be worth dividing the miscreants by a factor before gradient descent is applied and reapplying after. \n",
    "    \n",
    "* An alternative to feature scaling is to use mean normalisation. For the feature $x_j$ find the mean of all values and the deviation and calculate the mean normalised version of $x_j = \\frac{x_j - \\mu_j}{s_j} $ where $s_j$ is the standard deviation (max - min) and $\\mu_j$ is the average value of x. \n",
    "\n",
    "* To check that cnvergence is actually taking place a plot of J vs No. of iterations is useful to show if convergence is being reached. Andrew Ng provides some useful plots to show if convergence looks reasonable or not. This is helpful when choosing the learning rate. \n",
    "    \n",
    " \n",
    "## Decisions to be made \n",
    "\n",
    "1. Gradient Descent vs Normal Equation\n",
    "\n",
    "2. How much of data do we have? > 10k records? \n",
    "\n",
    "3. Do we need to scale or mean normalise our features? \n",
    "\n",
    "4. Do we need to reduce or increase number of features? \n",
    "\n",
    "5. How do we choose the learning rate? Try some values and compare the J vs Iter. plots to see which is the best. \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
